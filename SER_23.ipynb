{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f755e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import IPython\n",
    "import pyaudio\n",
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19d1d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying emotions in the RAVDESS dataset\n",
    "emotions = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "#Displaying the emotions to be observed\n",
    "obs_emo = ['calm', 'happy', 'fearful', 'disgust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf5b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the MLPClassifier\n",
    "model = MLPClassifier(alpha=0.01, batch_size='auto',epsilon=1e-08,\n",
    "                      hidden_layer_sizes=100, learning_rate='adaptive', learning_rate_init=0.001, max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfce9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recording the user's audio\n",
    "def recordAudio():\n",
    "    #CHUNCK is the number of frames the signals are split into\n",
    "    chunck = 1024 #Recoding in chuncks of 1024samples (block size)\n",
    "    sample_format = pyaudio.paInt16 #16bits per sample, data type format\n",
    "    #Each frame will have 1 sample as \"channels=1\"\n",
    "    channels = 1\n",
    "    #fs = sampling frequency\n",
    "    #fs is the number of audio samples collected in 1 second\n",
    "    fs = 48100  # Record at 44100 samples per 1 second //as per ravdess dataset the frequecy is 48kHz\n",
    "    seconds = 5\n",
    "    filename = \"Predict-Record-Audio.wav\"\n",
    "    \n",
    "    # Creating an interface to PortAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "    \n",
    "    print(\"Recording...\")\n",
    "    \n",
    "    #Start recording\n",
    "    stream = p.open(format=sample_format, channels=channels, rate=fs,\n",
    "                   frames_per_buffer=chunck, input=True)\n",
    "    \n",
    "    #Initializing an array to store frames\n",
    "    frames = []\n",
    "    \n",
    "    #Storing data in chuncks for 5seconds\n",
    "    for i in range(0, int(fs / chunck * seconds)):\n",
    "        data = stream.read(chunck)\n",
    "        ft = frames.append(data)\n",
    "   \n",
    "        \n",
    "    #Terminating and shutting down the stream/Recording\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    \n",
    "    #Terminating the PortAudio interface\n",
    "    p.terminate()\n",
    "    \n",
    "    print(\"Recording Complete.\")\n",
    "    \n",
    "    #Saving the recorded data as a .wav file\n",
    "    wf = wave.open(filename, 'wb')\n",
    "    wf.setnchannels(channels)\n",
    "    wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "    wf.setframerate(fs)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c91d6a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play the audio file\n",
    "def play(file):\n",
    "    chunck = 1024\n",
    "    wf = wave.open(file, 'rb')\n",
    "    \n",
    "    p = pyaudio.PyAudio()\n",
    "    \n",
    "    #To record or play audio, open a stream on the desired device\n",
    "    stream = p.open(format = p.get_format_from_width(wf.getsampwidth()),\n",
    "                   channels=wf.getnchannels(),\n",
    "                   rate = wf.getframerate(),\n",
    "                   output=True)\n",
    "    data = wf.readframes(chunck)\n",
    "    \n",
    "    while len(data) > 0:\n",
    "        stream.write(data)\n",
    "        data = wf.readframes(chunck)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    \n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "530dfffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float64\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "\n",
    "        \"\"\"\n",
    "        Short Time Fourier Transform (STFT). \n",
    "        STFTs can be used as a way of quantifying the change of a nonstationary signal's frequency\n",
    "        and phase content over time.\n",
    "        \"\"\"\n",
    "\n",
    "        if chroma:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        result = np.array([])\n",
    "        \n",
    "        if mfcc:\n",
    "            mfcc = np.mean(librosa.feature.mfcc(y=X, sr = sample_rate, n_mfcc = 40).T, axis=0)\n",
    "        result = np.hstack((result, mfcc))\n",
    "        \n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "        \n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(y = X, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf312dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data and extracting features for each sound file\n",
    "def load_data(test_size=0.2):\n",
    "    x, y = [], []\n",
    "    path = \"C:/Users/Geraldine/Desktop/LV400/SEM1/Design/final_design/Dataset/speech-emotion-recognition-ravdess-data/Actor_*/*.wav\"\n",
    "    for file in glob.glob(path):\n",
    "              \n",
    "        file_name = os.path.basename(file)\n",
    "        emo = emotions[file_name.split(\"-\")[2]]\n",
    "        \n",
    "        \n",
    "        if emo not in obs_emo:\n",
    "            continue\n",
    "        feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emo)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)\n",
    "# load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fc22cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into train and test datasets\n",
    "x_train, x_test, y_train, y_test = load_data(test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf37915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 154)\n",
      "Feature Extracted: 180\n"
     ]
    }
   ],
   "source": [
    "#Acquiring the shape of the training and testing datasets\n",
    "print((x_train.shape[0], x_test.shape[0]))\n",
    "    \n",
    "#Acquiring the number of features extracted\n",
    "print(f'Feature Extracted: {x_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f30b08cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 59.09%\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "model.fit(x_train, y_train)\n",
    "    \n",
    "#Predicting the test accuracy\n",
    "y_pred = model.predict(x_test)\n",
    "    \n",
    "#Getting the model's score/accuracy\n",
    "accuracy = accuracy_score(y_true = y_test, y_pred=y_pred)\n",
    "    \n",
    "#Displaying the accuracy\n",
    "print(\"Accuracy : {:.2f}%\".format(accuracy*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "debd5cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting on-predict audion\n",
    "\n",
    "def predictAudio(path):\n",
    "    file = \"C:/Users/Geraldine/Desktop/new-folder/audios/\"+path\n",
    "    print(file)\n",
    "    IPython.display.Audio(file)\n",
    "    x_predictAudio = []\n",
    "    featurePredictAudio = extract_feature(file, mfcc=True, chroma=True, mel=True) #extract features of recorded audio\n",
    "    x_predictAudio.append(featurePredictAudio)\n",
    "    y_predictAudio = model.predict(np.array(x_predictAudio))\n",
    "    print(y_predictAudio)\n",
    "    for i in y_predictAudio:\n",
    "        prediction = i\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf72c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recording the user's audio\n",
    "\n",
    "def record_predictAudio():\n",
    "    x_predictAudio = []\n",
    "    recordAudio()#Recording user's audio for prediction\n",
    "    file = \"C:/Users/Geraldine/Desktop/ATT-RT/Predict-Record-Audio.wav\" #file path to the recorded audio\n",
    "    \n",
    "    #Extracting the features of the recorded audio\n",
    "    featurePredAudio = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "    x_predictAudio.append(featurePredAudio)\n",
    "    y_predictAudio = model.predict(np.array(x_predictAudio))\n",
    "    print(y_predictAudio)\n",
    "    for i in y_predictAudio:\n",
    "        prediction = i\n",
    "        print(prediction)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "948c9f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ser_model.sav']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "#Saving the model\n",
    "filename = 'ser_model.sav'\n",
    "\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "008d3bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5909090909090909\n"
     ]
    }
   ],
   "source": [
    "#Loading the model from disc\n",
    "loaded_model = joblib.load(filename)\n",
    "result = loaded_model.score(x_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b6dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:9000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [02/May/2023 15:19:05] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/May/2023 15:19:05] \"GET /static/css/style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/May/2023 15:19:05] \"GET /static/aos-master/dist/aos.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/May/2023 15:19:05] \"GET /static/aos-master/dist/aos.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/May/2023 15:19:05] \"GET /static/img/take_record.gif HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/May/2023 15:19:05] \"GET /static/img/background2-01.svg HTTP/1.1\" 304 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/May/2023 15:19:13] \"GET /record_pred.html HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/May/2023 15:19:14] \"GET /static/css/style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/May/2023 15:19:14] \"GET /static/aos-master/dist/aos.css HTTP/1.1\" 304 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording Complete.\n",
      "['happy']\n",
      "happy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/May/2023 15:19:14] \"GET /static/aos-master/dist/aos.js HTTP/1.1\" 304 -\n"
     ]
    }
   ],
   "source": [
    "# app = Flask(__name__)\n",
    "# @app.route('/', methods=[\"POST\", \"GET\"])\n",
    "# def home():\n",
    "#     print('hell')\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     app.run(host=\"0.0.0.0\", port=5000, debug=True)\n",
    "\n",
    "from werkzeug.wrappers import Request, Response\n",
    "from flask import *\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return render_template(\"index.html\");\n",
    "\n",
    "@app.route(\"/record_pred.html\")\n",
    "def recPred():\n",
    "    predicted = record_predictAudio()\n",
    "    return render_template(\"record_pred.html\", predicted = predicted);\n",
    "\n",
    "# @app.route(\"/record_pred.html\")\n",
    "# def recPred():\n",
    "#     record_predictAudio()\n",
    "#     return render_template(\"record_pred.html\");\n",
    "\n",
    "@app.route(\"/input_aud.html\", methods=('GET', 'POST'))\n",
    "def inputPred():\n",
    "    if request.method == 'POST':\n",
    "        file_path = request.form.get('file')\n",
    "    print(\"C:/Users/Geraldine/Desktop/new-folder/audios\")\n",
    "    predicted = predictAudio(file_path)\n",
    "    return render_template(\"input_aud.html\", predicted = predicted);\n",
    "\n",
    "@app.route(\"/inputForm.html\", methods=('GET', 'POST'))\n",
    "def inputForm():\n",
    "    if request.method == 'POST':\n",
    "        file_path = request.form.get('file')\n",
    "        print(\"C:/Users/Geraldine/Desktop/new-folder/audios/\"+file_path)\n",
    "    return render_template(\"inputForm.html\");\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from werkzeug.serving import run_simple\n",
    "    run_simple('localhost', 9000, app)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
