{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b20640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import IPython\n",
    "import pyaudio\n",
    "import wave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cef20b",
   "metadata": {},
   "source": [
    "1 - Librosa provides building blocks to create audio retrieval system. \n",
    "2 - SoundFile reads and writes sound files supported in many platforms or operating systems. \n",
    "3 - OS provides functions for creating and removing a directory, fetching its contents, changing and identifying the current\n",
    "directory. 4 - GLOB returns all file paths matching a specific pattern. \n",
    "5 - IPython provides a rich toolkit to helps make the most out of using Python interactively. \n",
    "6 - Pyaudio is used to play and record audio in any operating system. \n",
    "7 - Wave writes audio data in raw format to a file-like object and reads the attributes of a WAV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "136e0425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying emotions in the RAVDESS dataset\n",
    "emotions = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "#Displaying the emotions to be observed\n",
    "obs_emo = ['calm', 'happy', 'fearful', 'disgust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bddfd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the MLPClassifier\n",
    "model = MLPClassifier(alpha=0.01, batch_size='auto',epsilon=1e-08,\n",
    "                      hidden_layer_sizes=100, learning_rate='adaptive', learning_rate_init=0.001, max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d78bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recording the user's audio\n",
    "def recordAudio():\n",
    "    #CHUNCK is the number of frames the signals are split into\n",
    "    chunck = 1024 #Recoding in chuncks of 1024samples (block size)\n",
    "    sample_format = pyaudio.paInt16 #16bits per sample, data type format\n",
    "    #Each frame will have 1 sample as \"channels=1\"\n",
    "    channels = 1\n",
    "    #fs = sampling frequency\n",
    "    #fs is the number of audio samples collected in 1 second\n",
    "    fs = 48100  # Record at 44100 samples per 1 second //as per ravdess dataset the frequecy is 48kHz\n",
    "    seconds = 5\n",
    "    filename = \"Predict-Record-Audio.wav\"\n",
    "    \n",
    "    # Creating an interface to PortAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "    \n",
    "    print(\"Recording...\")\n",
    "    \n",
    "    #Start recording\n",
    "    stream = p.open(format=sample_format, channels=channels, rate=fs,\n",
    "                   frames_per_buffer=chunck, input=True)\n",
    "    \n",
    "    #Initializing an array to store frames\n",
    "    frames = []\n",
    "    \n",
    "    #Storing data in chuncks for 5seconds\n",
    "    for i in range(0, int(fs / chunck * seconds)):\n",
    "        data = stream.read(chunck)\n",
    "        ft = frames.append(data)\n",
    "   \n",
    "        \n",
    "    #Terminating and shutting down the stream/Recording\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    \n",
    "    #Terminating the PortAudio interface\n",
    "    p.terminate()\n",
    "    \n",
    "    print(\"Recording Complete.\")\n",
    "    \n",
    "    #Saving the recorded data as a .wav file\n",
    "    wf = wave.open(filename, 'wb')\n",
    "    wf.setnchannels(channels)\n",
    "    wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "    wf.setframerate(fs)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1649921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play the audio file\n",
    "def play(file):\n",
    "    chunck = 1024\n",
    "    wf = wave.open(file, 'rb')\n",
    "    \n",
    "    p = pyaudio.PyAudio()\n",
    "    \n",
    "    #To record or play audio, open a stream on the desired device\n",
    "    stream = p.open(format = p.get_format_from_width(wf.getsampwidth()),\n",
    "                   channels=wf.getnchannels(),\n",
    "                   rate = wf.getframerate(),\n",
    "                   output=True)\n",
    "    data = wf.readframes(chunck)\n",
    "    \n",
    "    while len(data) > 0:\n",
    "        stream.write(data)\n",
    "        data = wf.readframes(chunck)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    \n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df122ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float64\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "\n",
    "        \"\"\"\n",
    "        Short Time Fourier Transform (STFT). \n",
    "        STFTs can be used as a way of quantifying the change of a nonstationary signal's frequency\n",
    "        and phase content over time.\n",
    "        \"\"\"\n",
    "\n",
    "        if chroma:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        result = np.array([])\n",
    "        \n",
    "        if mfcc:\n",
    "            mfcc = np.mean(librosa.feature.mfcc(y=X, sr = sample_rate, n_mfcc = 40).T, axis=0)\n",
    "        result = np.hstack((result, mfcc))\n",
    "        \n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "        \n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d781cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data and extracting features for each sound file\n",
    "def load_data(test_size=0.2):\n",
    "    x, y = [], []\n",
    "    path = \"C:/Users/Geraldine/Desktop/LV400/SEM1/Design/final_design/Dataset/speech-emotion-recognition-ravdess-data/Actor_*/*.wav\"\n",
    "    for file in glob.glob(path):\n",
    "              \n",
    "        file_name = os.path.basename(file)\n",
    "        emo = emotions[file_name.split(\"-\")[2]]\n",
    "        \n",
    "        \n",
    "        if emo not in obs_emo:\n",
    "            continue\n",
    "        feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emo)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)\n",
    "# load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45b9751e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "melspectrogram() takes 0 positional arguments but 1 positional argument (and 1 keyword-only argument) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Training the model is the program is started\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# def trainModel():\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Splitting the dataset into train and test datasets\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Acquiring the shape of the training and testing datasets\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m((x_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(test_size)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m emo \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m obs_emo:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m feature \u001b[38;5;241m=\u001b[39m \u001b[43mextract_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmfcc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchroma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m x\u001b[38;5;241m.\u001b[39mappend(feature)\n\u001b[0;32m     15\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(emo)\n",
      "Cell \u001b[1;32mIn[6], line 25\u001b[0m, in \u001b[0;36mextract_feature\u001b[1;34m(file_name, mfcc, chroma, mel)\u001b[0m\n\u001b[0;32m     22\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((result, chroma))\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mel:\n\u001b[1;32m---> 25\u001b[0m         mel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmelspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     26\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((result, mel))\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mTypeError\u001b[0m: melspectrogram() takes 0 positional arguments but 1 positional argument (and 1 keyword-only argument) were given"
     ]
    }
   ],
   "source": [
    "# Training the model is the program is started\n",
    "# def trainModel():\n",
    "    \n",
    "#Splitting the dataset into train and test datasets\n",
    "x_train, x_test, y_train, y_test = load_data(test_size = 0.2)\n",
    "    \n",
    "#Acquiring the shape of the training and testing datasets\n",
    "print((x_train.shape[0], x_test.shape[0]))\n",
    "    \n",
    "#Acquiring the number of features extracted\n",
    "print(f'Feature Extracted: {x_train.shape[1]}')\n",
    "    \n",
    "#Training the model\n",
    "model.fit(x_train, y_train)\n",
    "    \n",
    "#Predicting the test accuracy\n",
    "y_pred = model.predict(x_test)\n",
    "    \n",
    "#Getting the model's score/accuracy\n",
    "accuracy = accuracy_score(y_true = y_test, y_pred=y_pred)\n",
    "    \n",
    "#Displaying the accuracy\n",
    "print(\"Accuracy : {:.2f}%\".format(accuracy*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c29f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting on-predict audion\n",
    "\n",
    "def predictAudio(path):\n",
    "    file = \"C:/Users/Geraldine/Desktop/new-folder/audios/\"+path\n",
    "    print(file)\n",
    "    IPython.display.Audio(file)\n",
    "    x_predictAudio = []\n",
    "    featurePredictAudio = extract_feature(file, mfcc=True, chroma=True, mel=True) #extract features of recorded audio\n",
    "    x_predictAudio.append(featurePredictAudio)\n",
    "    y_predictAudio = model.predict(np.array(x_predictAudio))\n",
    "    print(y_predictAudio)\n",
    "    for i in y_predictAudio:\n",
    "        prediction = i\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recording the user's audio\n",
    "\n",
    "def record_predictAudio():\n",
    "    x_predictAudio = []\n",
    "    recordAudio()#Recording user's audio for prediction\n",
    "    file = \"C:/Users/Geraldine/Desktop/ATT-RT/Predict-Record-Audio.wav\" #file path to the recorded audio\n",
    "    \n",
    "    #Extracting the features of the recorded audio\n",
    "    featurePredAudio = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "    x_predictAudio.append(featurePredAudio)\n",
    "    y_predictAudio = model.predict(np.array(x_predictAudio))\n",
    "    print(y_predictAudio)\n",
    "    for i in y_predictAudio:\n",
    "        prediction = i\n",
    "        print(prediction)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "#Saving the model\n",
    "filename = 'ser_model.sav'\n",
    "\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e2281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model from disc\n",
    "loaded_model = joblib.load(filename)\n",
    "result = loaded_model.score(x_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e43a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app = Flask(__name__)\n",
    "# @app.route('/', methods=[\"POST\", \"GET\"])\n",
    "# def home():\n",
    "#     print('hell')\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     app.run(host=\"0.0.0.0\", port=5000, debug=True)\n",
    "\n",
    "from werkzeug.wrappers import Request, Response\n",
    "from flask import *\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return render_template(\"index.html\");\n",
    "\n",
    "@app.route(\"/record_pred.html\")\n",
    "def recPred():\n",
    "    predicted = record_predictAudio()\n",
    "    return render_template(\"record_pred.html\", predicted = predicted);\n",
    "\n",
    "# @app.route(\"/record_pred.html\")\n",
    "# def recPred():\n",
    "#     record_predictAudio()\n",
    "#     return render_template(\"record_pred.html\");\n",
    "\n",
    "@app.route(\"/input_aud.html\", methods=('GET', 'POST'))\n",
    "def inputPred():\n",
    "    if request.method == 'POST':\n",
    "        file_path = request.form.get('file')\n",
    "    print(\"C:/Users/Geraldine/Desktop/new-folder/audios\")\n",
    "    predicted = predictAudio(file_path)\n",
    "    return render_template(\"input_aud.html\", predicted = predicted);\n",
    "\n",
    "@app.route(\"/inputForm.html\", methods=('GET', 'POST'))\n",
    "def inputForm():\n",
    "    if request.method == 'POST':\n",
    "        file_path = request.form.get('file')\n",
    "        print(\"C:/Users/Geraldine/Desktop/new-folder/audios/\"+file_path)\n",
    "    return render_template(\"inputForm.html\");\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from werkzeug.serving import run_simple\n",
    "    run_simple('localhost', 9000, app)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
